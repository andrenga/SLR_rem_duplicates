{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLR: Merge bibliometric files and remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook processes bibliometric datasets from **Web of Science**, **Scopus**, and **ProQuest**. Follow these steps to preprocess, merge, deduplicate, and analyze the data.\n",
    "\n",
    "**NOTE:** The code only supports **CSV** and **Excel** formats for the bibliometric data.\n",
    "\n",
    "### Steps to Use:\n",
    "\n",
    "1. **Provide the Filepaths:**\n",
    "   - Assign the file paths for your datasets to the following variables in the designated cell:\n",
    "     - `FILE_PATH_WOS` → Path to the Web of Science dataset.\n",
    "     - `FILE_PATH_SD` → Path to the Scopus dataset.\n",
    "     - `FILE_PATH_PQ` → Path to the ProQuest dataset.\n",
    "\n",
    "2. **Run All Cells:**\n",
    "   - Execute all the code cells in the notebook sequentially to process the data.\n",
    "\n",
    "3. **View the Results:**\n",
    "   - The notebook will display:\n",
    "     - The number of studies before and after duplicate removal.\n",
    "     - The number of duplicates removed.\n",
    "     - The count of studies from each source (Web of Science, Scopus, ProQuest).\n",
    "     - The complete deduplicated dataset for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_WOS = \"\"\n",
    "FILE_PATH_SD = \"\"\n",
    "FILE_PATH_PQ = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globally suppress the SettingsWithCopyWarning from the Pandas library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress the SettingWithCopyWarning globally\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Read bibliometric files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *read_file* takes a file path (*filename*) as input and reads it into a pandas DataFrame. It supports two file formats:\n",
    "\n",
    "- **CSV files**: If the file has a .csv extension, it is read using pd.read_csv.\n",
    "- **Excel files**: If the file doesn't have a .csv extension, it attempts to read it as an Excel file using pd.read_excel. If this fails (e.g., due to an unsupported format), it prints an error message: \"Wrong file format\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a file into a Pandas DataFrame. Supports CSV and Excel formats.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the file to read.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the data from the file.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file format is unsupported or cannot be read.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the file is a CSV by its extension\n",
    "    if (filename.endswith(\".csv\")):\n",
    "        return pd.read_csv(filename)\n",
    "    else:\n",
    "        # Attempt to read as an Excel file if not a CSV\n",
    "        try:\n",
    "            return pd.read_excel(filename)\n",
    "        except:\n",
    "            # Print an error message if the file format is unsupported or cannot be read\n",
    "            print(\"Wrong file format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *read_files* function is designed to read the three data files into Pandas DataFrames. It uses the *read_file* function to handle the file reading. The function returns a tuple containing these three DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files() -> tuple:\n",
    "    \"\"\"\n",
    "    Reads three specific bibliometric datasets into Pandas DataFrames.\n",
    "\n",
    "    Uses the `read_file` function to read the bibliometric files specified by the following variables:\n",
    "    - FILE_PATH_WOS: Path to the Web of Science dataset.\n",
    "    - FILE_PATH_SD: Path to the Scopus dataset.\n",
    "    - FILE_PATH_PQ: Path to the ProQuest dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames:\n",
    "            - df_wos (pd.DataFrame): DataFrame containing Web of Science data.\n",
    "            - df_scopus (pd.DataFrame): DataFrame containing Scopus data.\n",
    "            - df_proquest (pd.DataFrame): DataFrame containing ProQuest data.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_wos = read_file(FILE_PATH_WOS)\n",
    "    df_scopus = read_file(FILE_PATH_SD)\n",
    "    df_proquest = read_file(FILE_PATH_PQ)\n",
    "\n",
    "    return df_wos, df_scopus, df_proquest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Adjust DataFrame Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code contains three functions that preprocess bibliometric datasets from the different sources — *Web of Science*, *Scopus*, and *ProQuest* — to standardize and clean the data for analysis. Each function takes a Pandas DataFrame as input and returns a cleaned DataFrame with consistent column names, filtered content, and transformed values.\n",
    "\n",
    "More precisely, for each DataFrame, the code adjust the format and structure, so that all DataFrames are identical, allowing for merging later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_wos(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses and standardizes bibliometric data from the Web of Science database.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw Web of Science data as a Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and standardized DataFrame containing the following columns:\n",
    "            - Authors: Author names (spaces removed, lowercase).\n",
    "            - Title: Article title (lowercase).\n",
    "            - Year: Year of publication.\n",
    "            - Database: Fixed value 'Web of Science'.\n",
    "            - Citations: Number of citations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add column specifying database\n",
    "    df[\"Database\"] = \"Web of Science\"\n",
    "\n",
    "    # Only keep relevant columns: authors, title and year\n",
    "    df = df[[\"Author Full Names\", \"Article Title\", \"Publication Year\", \"Database\", \"Times Cited, WoS Core\"]]\n",
    "\n",
    "    # Rename columns to standard names\n",
    "    df = df.rename(columns={\"Author Full Names\": \"Authors\", \"Article Title\": \"Title\",\n",
    "                            \"Publication Year\": \"Year\", \"Times Cited, WoS Core\": \"Citations\"})\n",
    "\n",
    "    # Remove spaces\n",
    "    df[\"Authors\"] = df[\"Authors\"].str.replace(' ', '', regex=False)\n",
    "\n",
    "    # Turn strings to lowercase\n",
    "    df[\"Authors\"] = df[\"Authors\"].str.lower()\n",
    "    df[\"Title\"] = df[\"Title\"].str.lower()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_scopus(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses and standardizes bibliometric data from the Scopus database.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw Scopus data as a Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and standardized DataFrame containing the following columns:\n",
    "            - Authors: Author names (spaces and parentheses removed, lowercase).\n",
    "            - Title: Article title (lowercase).\n",
    "            - Year: Year of publication.\n",
    "            - Database: Source of the data (formerly 'Source'), which is the fixed value 'Scopus'.\n",
    "            - Citations: Number of citations (formerly 'Cited by').\n",
    "    \"\"\"\n",
    "\n",
    "    # Only keep relevant columns: authors, title and year\n",
    "    df = df[[\"Author full names\", \"Title\", \"Year\", \"Source\", \"Cited by\"]]\n",
    "\n",
    "    # Rename columns to standard names\n",
    "    df = df.rename(columns={\"Author full names\": \"Authors\", \"Cited by\": \"Citations\", \"Source\": \"Database\"})\n",
    "\n",
    "    # Remove spaces\n",
    "    df[\"Authors\"] = df[\"Authors\"].str.replace(' ', '', regex=False)\n",
    "\n",
    "    # Turn strings to lowercase\n",
    "    df[\"Authors\"] = df[\"Authors\"].str.lower()\n",
    "    df[\"Title\"] = df[\"Title\"].str.lower()\n",
    "\n",
    "    # Remove parentheses and their contents in column \"Authors\"\n",
    "    df[\"Authors\"] = df[\"Authors\"].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_proquest(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses and standardizes bibliometric data from the ProQuest database.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw ProQuest data as a Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and standardized DataFrame containing the following columns:\n",
    "            - Authors: Author names (spaces removed, lowercase).\n",
    "            - Title: Article title (lowercase).\n",
    "            - Year: Year of publication (extracted from 'PubDate').\n",
    "            - Database: Source of the data (formerly 'Source'), which is the fixed value 'ProQuest'.\n",
    "            - Citations: Number of citations (formerly 'Cited by').\n",
    "    \"\"\"\n",
    "\n",
    "    # Only keep relevant columns: authors, title and year\n",
    "    df = df[[\"Author\", \"Title\", \"PubDate\", \"Source\", \"Cited by\"]]\n",
    "\n",
    "    # Extract year from column \"PubDate\"\n",
    "    df[\"PubDate\"] = pd.to_datetime(df[\"PubDate\"])\n",
    "    df[\"PubDate\"] = df[\"PubDate\"].dt.year\n",
    "\n",
    "    # Rename columns to standard names\n",
    "    df = df.rename(columns={\"Author\": \"Authors\", \"PubDate\": \"Year\", \"Cited by\": \"Citations\", \"Source\": \"Database\"})\n",
    "\n",
    "    # Remove spaces\n",
    "    df[\"Authors\"] = df[\"Authors\"].str.replace(' ', '', regex=False)\n",
    "\n",
    "    # Turn strings to lowercase\n",
    "    df[\"Authors\"] = df[\"Authors\"].str.lower()\n",
    "    df[\"Title\"] = df[\"Title\"].str.lower()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Merge DataFrame to a single DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *merge_dfs* function combines data from the three Pandas DataFrames — *df_wos*, *df_scopus*, and *df_proquest* — into a single DataFrame using Pandas' concat function. It creates a unified dataset by stacking the rows from the input DataFrames.\n",
    "\n",
    "Hence, the merged DataFrame contains all the studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(df_wos: pd.DataFrame, df_scopus: pd.DataFrame, df_proquest: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges three bibliometric DataFrames into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df_wos (pd.DataFrame): DataFrame containing Web of Science data.\n",
    "        df_scopus (pd.DataFrame): DataFrame containing Scopus data.\n",
    "        df_proquest (pd.DataFrame): DataFrame containing ProQuest data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A merged DataFrame containing all rows from the three input DataFrames, \n",
    "                      with reset index for consistency.\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.concat([df_wos, df_scopus, df_proquest], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Remove Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *rem_duplicates* function removes duplicate records from the merged Pandas DataFrame based on the combination of *Authors*, *Title*, and *Year* columns.\n",
    "\n",
    "It ensures consistent text formatting by normalizing special characters and reorders the columns for better readability. The function also provides a summary of the number of unique rows compared to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_duplicates(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Removes duplicate records from a DataFrame and provides a summary.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing bibliometric data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - df_uniques (pd.DataFrame): DataFrame with duplicates removed.\n",
    "            - df_uniques.shape[0]: Number of unique rows in the DataFrame.\n",
    "            - df.shape[0]: Number of rows in the original DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle variations in special characters\n",
    "    df[\"Authors\"] = df[\"Authors\"].apply(unidecode)\n",
    "    df[\"Title\"] = df[\"Title\"].apply(unidecode)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.sort_values(by=\"Citations\", ascending=False)\n",
    "    df_uniques = df.drop_duplicates([\"Authors\", \"Title\", \"Year\"], keep=\"first\")\n",
    "\n",
    "    # Reorder columns to make \"Title\" the first column\n",
    "    df_uniques = df_uniques[[\"Title\"] + [col for col in df_uniques.columns if col != \"Title\"]]\n",
    "\n",
    "    df_uniques = df_uniques.sort_values(by=\"Title\")\n",
    "\n",
    "    return df_uniques, df_uniques.shape[0], df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *main* function performs all necessary steps and calls all functions.\n",
    "\n",
    "More precisely, the function orchestrates the end-to-end process of reading, preprocessing, merging, deduplicating, and summarizing bibliometric data from multiple sources: Web of Science, Scopus, and ProQuest. It ensures data consistency and provides a detailed summary of the dataset before and after removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process and summarize bibliometric data from multiple sources.\n",
    "\n",
    "    Workflow:\n",
    "    1. Read raw data from Web of Science, Scopus, and ProQuest.\n",
    "    2. Preprocess and standardize the individual datasets.\n",
    "    3. Merge the datasets into a unified DataFrame.\n",
    "    4. Remove duplicate studies and summarize results.\n",
    "    5. Print the deduplicated DataFrame and summary statistics.\n",
    "\n",
    "    Outputs:\n",
    "        - Number of studies before and after deduplication.\n",
    "        - Number of duplicates removed.\n",
    "        - Breakdown of studies by source (Web of Science, Scopus, ProQuest).\n",
    "        - Full deduplicated DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_wos, df_scopus, df_proquest = read_files()\n",
    "    \n",
    "    df_wos_adj = adjust_wos(df_wos)\n",
    "    df_scopus_adj = adjust_scopus(df_scopus)\n",
    "    df_proquest_adj = adjust_proquest(df_proquest)\n",
    "\n",
    "    df_merged = merge_dfs(df_wos_adj, df_scopus_adj, df_proquest_adj)\n",
    "\n",
    "    df_uniques, df_uniques_size, df_size = rem_duplicates(df_merged)\n",
    "\n",
    "    df_wos = df_uniques[df_uniques[\"Database\"] == \"Web of Science\"]\n",
    "    df_scopus = df_uniques[df_uniques[\"Database\"] == \"Scopus\"]\n",
    "    df_proquest = df_uniques[df_uniques[\"Database\"] == \"ProQuest\"]\n",
    "\n",
    "    print(f\"Number of studies before removal of duplicates studies: {df_size}\")\n",
    "    print(f\"Number of studies removed: {df_size - df_uniques_size}\")\n",
    "    print(f\"Number of studies after automatic removal of duplicates studies (automatic): {df_uniques_size}\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Number of studied from Web of Science: {df_wos.shape[0]}\")\n",
    "    print(f\"Number of studied from Scopus: {df_scopus.shape[0]}\")\n",
    "    print(f\"Number of studied from ProQuest: {df_proquest.shape[0]}\")\n",
    "    print(\"------------------------------\")\n",
    "    print()\n",
    "    print(df_uniques.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
